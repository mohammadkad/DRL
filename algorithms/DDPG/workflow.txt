DDPG Workflow Diagram
Mohammad Kadkhodaei
1404-05-27
┌───────────────────────┐    ┌───────────────────────┐
│      Environment      │    │      DDPG Agent       │
└───────────┬───────────┘    └───────────┬───────────┘
            │                            │
            │ 1. Current State (s)       │
            ├───────────────────────────>│
            │                            │
            │                            │ 2. Select Action (a = μ(s) + noise)
            │                            ├───────────────────────────┐
            │                            │                           │
            │ 3. Execute Action (a)      │                           │
            │<──────────────────────────-┤                           │
            │                            │                           │
            │ 4. Observe (r, s', done)   │                           │
            ├───────────────────────────>│                           │
            │                            │                           │
            │                            │ 5. Store (s,a,r,s',done)  │
            │                            │ in Replay Buffer          │
            │                            │                           │
            │                            │ 6. Sample Random Batch    │
            │                            │ from Replay Buffer        │
            │                            │                           │
            │                            │ 7. Update Networks:       │
            │                            │    - Compute targets      │
            │                            │    - Update critic        │
            │                            │    - Update actor         │
            │                            │    - Update target nets   │
            │                            │                           │
            └────────────────────────────┴──────────────────────────-┘

Key Steps in Workflow:

1- Environment provides current state

2- Agent selects action using actor network + exploration noise

3- Action is executed in environment

4-Environment returns reward, next state, and done flag

5- Transition is stored in replay buffer

6- Random batch is sampled from buffer for learning

7- Networks are updated:

  Critic learns to better estimate Q-values

  Actor learns to select better actions

  Target networks are softly updated

8- Process repeats
